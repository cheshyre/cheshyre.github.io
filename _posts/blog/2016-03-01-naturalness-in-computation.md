---
title: A Practical Motivation for Naturalness
categories: blog
layout: default
---
If you have spent any amount of time working in the field of theoretical physics, you are almost certainly aware of the concept of naturalness. Generally speaking, in physics, we expect constants and observables to be of a "natural" size relative to some dimensionless scale that characterizes the system we are modeling. What this actually means is that we expect constants and observables to be of roughly the same size as the natural constants that characterize the system. A very simple example of this is how special relativity is approached in mechanics. Instead of referring to the velocities of inertial frames as 200 million meters per second (or even worse, something stupid like miles per hour), in the context of relativity, we define velocities as being some fraction of the speed of light in the vacuum (roughly 300 million meters per second). This is because the behavior of particles in systems where special relativity applies is governed by their speed _relative_ to the speed of light.

So this is great and all, but isn't this just something we do for convenience? Actually, naturalness is an intuition physicists have that seems to be applicable to all sorts of systems. Or, if you want to be optimistic about it, it seems to be deeply engrained into how the world actaully works, that when properly considering the scales for our problems, we get results that are roughly of order unity. Regardless, naturalness is not just a convention physicists have to simplify notation. Frequently, it is stated that physicists expect their results to be of natural size (even in places where neither theory nor experiment have really found anything to suggest it). And it turns out that this is always true.

So now we understand what naturalness is, but all of this seems kind of abstract and hand-wavey. How can we think about this in a more practical way? Why should we choose to work at natural scales besides this physical intuition that most of probably don't have? One good reason I can come up with is that it makes computations much easier and, more importantly, much more accurate. In some of my recent work, I was reminded just how terrible accumulated floating point errors are in complex calculations. Iterative approximation methods often have two competing sources of error: error due to the approximation (which we can systematically decrease by choosing to iterate more) and an error due to the accumulation of floating point errors (which will go between something like machine-precision * sqrt(N) and machine-precision * N depending on the nature of the algorithm). 

What does this mean for calculations? Basically, in anything quantitative, we define the precision of a result in terms of relative precision. This means 1.001 is more precise than 0.00003, for example. To cut a long story short, if we choose to calculate at natural scales, we can get high relative precision without wandering into the territory where our method error and floating point error become of roughly equal size. For many fields of physics, it's not even practical to try and do calculations in terms of SI units because all the values are either too large or small for good digital representation. Working at natural sizes makes computational work not only feasible, but very precise (if done correctly).